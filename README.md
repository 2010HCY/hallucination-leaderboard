# Hallucination Leaderboard

Public LLM leaderboard computed using our Hallucination Evaluation Model. We plan to update this monthly, or sooner when appropriate.

Last updated on November 1st, 2023

|Model|Accuracy|Hallucination Rate|Average Summary Length (Words)|Answer Rate|
|----|----|----|----|----|
|GPT 4|97.0 %|3.0 %|81.1|100 %|
|GPT3.5|96.5 %|3.5%|84.1|99.6 %|
|Llama 2 70B|94.9 %|5.1 %|84.9|99.9 %|
|Llama 2 7B|94.4 %|5.6 %|119.9|99.6 %|
|Llama 2 13B|94.1 %|5.9 %|82.1|99.8 %|
|Cohere-Chat|92.5 %|7.5 %|74.4|98.0 %|
|Cohere|91.5 %|8.5 %|59.8|99.8 %|
|Anthropic Claude 2|91.5 %|8.5 %|87.5|99.3 %|
|Mistral 7B|90.6 %|9.4 %|96.1|98.7 %|
|Google Palm|87.9 %|12.1 %|36.2|92.4 %|
|Google Palm-Chat|72.8 %|27.2 %|221.1|88.8 %|

**TODO**
* Link to Github model. Replicate instructions on how to use model
* Describe each llm version used above.

